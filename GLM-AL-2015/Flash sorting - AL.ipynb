{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See environment_setup.README (below) for instructions about the use of the DC3_plots_NALMA script. It is a version of the script used to process the DC3 dataset as in Barth et al. (2015, BAMS) and Bruning and Thomas (2015, JGR).\n",
    "\n",
    "The flash sorting infrastructure is modular. This script uses the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN\">DBSCAN algorithm </a> as implemented in the <a href=\"http://scikit-learn.org\">scikit-learn</a> machine-learning library. In order to manage the $N^2$ efficiency of the underlying DBSCAN implementation, data are clustered in pairs of `thresh_duration` chunks.\n",
    "\n",
    "The script is configurable in a few places. \n",
    "- `base_sort_dir` sets the path where \n",
    "- `center_ID` chooses a network center. The centers are defined in the `centers` dictionary. The ID is used later when constructing output filenames, too.\n",
    "- The `params` dictionary configures the flash sorting algorithm. Of particular importance are the following.\n",
    "  - `stations`: sets the (min, max) number of stations that must participate in each solution for it to count. Max should be larger than the number of stations. Min should be six or seven, depending on the number of stations.\n",
    "  - `chi2`: sets the (min, max) chi-squared value. The minimum should be zero, while a good maximum to start with is 1.0.\n",
    "  - `distance`: maximum distance between a source and its closest neighbor before a new flash is started\n",
    "  - `thresh_critical_time`: maximum temporal separation between a source and its closest neighbor before a new flash is started\n",
    "  - `thresh_duration`: All flashes should be last less than or equal to this number of seconds. All flashes of duration < `thresh_duration` are guaranteed to remain clustered. An occasional lucky flash of duration =  2 \\* `thresh_duration` is possible.\n",
    "\n",
    "The script is broken into three sections.\n",
    "- Run the flash sorting, which creates HDF5 data files with VHF source data, their flash IDs, and a matching flash data table.\n",
    "- Grab the flash-sorted files and create CF-compliant NetCDF grids\n",
    "- Grab the grids and create PDF images of each grid\n",
    "\n",
    "The grid spacing, boundaries, and frame intervals are configured at the begining of the gridding section of the script. This script creates regularly-spaced lat/lon grids, with the center grid cell size calculated to match the specified `dx_km` and `dy_km`. It is also possible to grid directly in a map projection of choice by changing `proj_name`, as well as `x_name` and `y_name` in the call to `make_plot`. For instance, a geostationary projection can be obtained with `proj='geos'` as described in the [documentation for the proj4 coordinate system library](http://trac.osgeo.org/proj/wiki/proj%3Dgeos).\n",
    "\n",
    "The PDF images are created as small-multiple plots, with the number of columns given by `n_cols` at the beginning of the plotting section.\n",
    "\n",
    "An example of reading and working with the resulting data files is found in the \"Reading the flash-sorted files.ipynb\"\n",
    "\n",
    "As described below, additional scripts perform follow-on analysis.\n",
    "- Assigning NLDN strokes to the best-matching flash\n",
    "- Using a storm cell or storm region polygon to subset some flashes from the data files.\n",
    "    - Creating time series plots of moments of the flash size distribution\n",
    "    - Creating ASCII files of flash size and rate statistics\n",
    "\n",
    "\n",
    "The IOP bounding box file included here is a rectangular lat/lon box, but the underlying code works with arbitrary polygons. Adapting the existing code to polygons is mostly a matter of reading in polygon vertices and sending its vertices instead of those for a rectangle.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 September 2015 \n",
      "Eric Bruning eric.bruning@ttu.edu\n",
      "\n",
      "This document provides details about running a flash-sorting analysis, including\n",
      "producing the flash time series statistics. These scripts are largely those used\n",
      "for processing the DC3 dataset at TTU, and are suitable for (re)processing a large number of cases.\n",
      "\n",
      "Python setup\n",
      "------------\n",
      "\n",
      "This analysis is run using the Anaconda Python distribution, with the primary\n",
      "needs being numpy, scipy, matplotlib, pupynere, and pyproj.\n",
      "\n",
      "After installing anaconda to your home directory, go to your home directory,\n",
      "without anaconda already in $PATH\n",
      "\n",
      "./anaconda/bin/conda create -n LMA --clone root\n",
      "\n",
      "cd anaconda/envs/LMA/ source ./anaconda/bin/activate LMA\n",
      "\n",
      "This sets up an environment with a python environment with just the necessary pieces for this LMA analysis.\n",
      "\n",
      "then pip install git+http://github.com/deeplycloudy/lmatools\n",
      "git+http://github.com/deeplycloudy/stormdrain\n",
      "\n",
      "If you have to pause and return later then simply cd home and\n",
      "\n",
      "source ./anaconda/bin/activate LMA\n",
      "\n",
      "Running the analysis\n",
      "--------------------\n",
      "\n",
      "Finally, change to the directory where you have the analysis scripts. It should\n",
      "contain 'figures-length' and 'results' directories into which output will be placed.\n",
      "\n",
      "First sort flashes to HDF5 files and create 1 km, 1 min grids:\n",
      "\n",
      "python DC3_plots_NALMA.py /data/GLM-wkshp/20090410/LMA_files/*.dat.gz\n",
      "\n",
      "You will need to edit the path \"base_sort_dir\" near the top of the script.\n",
      "\n",
      "To add in NLDN data, run, for each H5 file,\n",
      "\n",
      "python NLDN_matching.py ../20090410/NLDN_files/Nstroke20090410_daily_v1_lit.raw  results/h5_files/2009/Apr/10/LYLOUT_090410_160000_3600.dat.flash.h5\n",
      "\n",
      "Run the following, and capture the output to output.txt to get the flash size\n",
      "time series stats, along with other run info \n",
      "\n",
      "You will need to edit the \"path_to_sort_results\" to point to the 'results' directory.\n",
      "\n",
      "python DC3-IOP-hires-stats.py IOPsupercell18-AL-20090410-boundingbox.txt > figures-length/IOPsupercell18-output.txt\n",
      "\n",
      "The energy-stats.py script is used by DC3-IOP-stats.py; you shouldn't run it\n",
      "directly.\n",
      "\n",
      "harvest_flash_timeseries.py will produce a CSV file of flash timeseries data from\n",
      "the captured output of the main script. \n",
      "\n",
      "python harvest_flash_timeseries.py figures-length/IOPsupercell18-output.txt \n",
      "\n",
      "make_movies.py can be used with the ffmpeg utility to stitch together the grid images into movies.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat /data/GLM-wkshp/flashsort/environment_setup.README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
